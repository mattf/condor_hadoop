# Submit w/ condor_submit -a NameNodeAddress=<address>
# e.g. <address> = hdfs://$HOSTNAME:9001

+HadoopType = "JobTracker"

cmd = mapred_jobtracker.sh
args = $(HadoopBinTarBall) $(NameNodeAddress)

transfer_input_files = $(HadoopBinTarBall)

# RFE: Ability to get output files even when job is removed
#transfer_output_files = logs.tgz
#transfer_output_remaps = "logs.tgz logs.$(cluster).tgz"

output = jobtracker.$(cluster).out.txt
error = jobtracker.$(cluster).err.txt
log = jobtracker.$(cluster).log.txt


kill_sig = SIGTERM

# prefer machines with more memory
# there may be other preferences based on locality.
rank = memory

# Want chirp functionality
+WantIOProxy = TRUE

should_transfer_files = yes
when_to_transfer_output = on_exit

requirements = HasJava =?= TRUE

# useful for testing on personal condor setups
getenv= $ENV(DEBUG) =?= TRUE

queue
