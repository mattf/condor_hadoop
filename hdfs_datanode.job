# Submit w/ condor_submit -a NameNodeAddress=<address>
# e.g. <address> = hdfs://$HOSTNAME:2007

+HadoopType = "DataNode"

cmd = hdfs_datanode.sh
args = $(HadoopBinTarBall) $(NameNodeAddress)

transfer_input_files = $(HadoopBinTarBall)

# RFE: Ability to get output files even when job is removed
#transfer_output_files = logs.tgz
#transfer_output_remaps = "logs.tgz logs.$(cluster).tgz"

output = datanode.$(cluster).out.txt
error = datanode.$(cluster).err.txt
log = datanode.$(cluster).log.txt

kill_sig = SIGTERM

# prefer machines with more disk space (Network is important too!)
# So in reality it would be the sum of it's parts, but we would need 
# custom startd attributes to enable.
# .4*disk + .5*bandwith + .1*locality 
rank = disk

# This is a temporary method which says we want the data nodes to have at least 
# 65% of the nodes disk space.  This will prevent other data nodes from landing on the 
# same machine while still allowing other non data intensive jobs to land  
request_disk = floor(.65 * Target.TotalDisk)

# Want chirp functionality
+WantIOProxy = TRUE

should_transfer_files = yes
when_to_transfer_output = on_exit

requirements = HasJava =?= TRUE 

# useful for testing on personal condor setups
# getenv= $ENV(DEBUG) =?= TRUE

queue
